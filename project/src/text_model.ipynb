{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 1000, 'display.max_colwidth', 1000, 'display.max_rows',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>btc-usdopen</th>\n",
       "      <th>clfopen</th>\n",
       "      <th>tslaopen</th>\n",
       "      <th>gspcopen</th>\n",
       "      <th>gsptseopen</th>\n",
       "      <th>ixicopen</th>\n",
       "      <th>btc-usddelta</th>\n",
       "      <th>btc-usddirection</th>\n",
       "      <th>clfdelta</th>\n",
       "      <th>clfdirection</th>\n",
       "      <th>tsladelta</th>\n",
       "      <th>tsladirection</th>\n",
       "      <th>gspcdelta</th>\n",
       "      <th>gspcdirection</th>\n",
       "      <th>gsptsedelta</th>\n",
       "      <th>gsptsedirection</th>\n",
       "      <th>ixicdelta</th>\n",
       "      <th>ixicdirection</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>content</th>\n",
       "      <th>retweetnum</th>\n",
       "      <th>likenum</th>\n",
       "      <th>userhandle</th>\n",
       "      <th>content1</th>\n",
       "      <th>tweet_day</th>\n",
       "      <th>day</th>\n",
       "      <th>score</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>combined</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-26 09:30:00</td>\n",
       "      <td>38421.363281</td>\n",
       "      <td>86.43</td>\n",
       "      <td>952.429993</td>\n",
       "      <td>4408.430176</td>\n",
       "      <td>20781.300781</td>\n",
       "      <td>13868.869141</td>\n",
       "      <td>0.158282</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.103958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-01-25 16:00:26+00:00</td>\n",
       "      <td>The numbers are in, and the world still loves Adele—even at her worst https://t.co/RIl3yyFA4V https://t.co/mbI3YbCZrI</td>\n",
       "      <td>91</td>\n",
       "      <td>566</td>\n",
       "      <td>business</td>\n",
       "      <td>the number are in and the world still love adeleeven at her worst</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>[0.209, 0.51, 0.281, 0.1027]</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date   btc-usdopen  clfopen    tslaopen     gspcopen  \\\n",
       "0  2022-01-26 09:30:00  38421.363281    86.43  952.429993  4408.430176   \n",
       "\n",
       "     gsptseopen      ixicopen  btc-usddelta  btc-usddirection  clfdelta  \\\n",
       "0  20781.300781  13868.869141      0.158282               2.0 -0.103958   \n",
       "\n",
       "   clfdirection  tsladelta  tsladirection  gspcdelta  gspcdirection  \\\n",
       "0           0.0        NaN            NaN        NaN            NaN   \n",
       "\n",
       "   gsptsedelta  gsptsedirection  ixicdelta  ixicdirection  \\\n",
       "0          NaN              NaN        NaN            NaN   \n",
       "\n",
       "                  tweet_date  \\\n",
       "0  2022-01-25 16:00:26+00:00   \n",
       "\n",
       "                                                                                                                 content  \\\n",
       "0  The numbers are in, and the world still loves Adele—even at her worst https://t.co/RIl3yyFA4V https://t.co/mbI3YbCZrI   \n",
       "\n",
       "   retweetnum  likenum userhandle  \\\n",
       "0          91      566   business   \n",
       "\n",
       "                                                             content1  \\\n",
       "0   the number are in and the world still love adeleeven at her worst   \n",
       "\n",
       "  tweet_day        day                         score  negative  neutral  \\\n",
       "0   Tuesday  Wednesday  [0.209, 0.51, 0.281, 0.1027]     0.209     0.51   \n",
       "\n",
       "   positive  combined  sentiment  \n",
       "0     0.281    0.1027          2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/df_raw.csv')\n",
    "col=[i.replace(' ','') for i in df.columns]\n",
    "col=[i.replace('=','') for i in col]\n",
    "col=[str.lower(i) for i in col]\n",
    "df.columns=col\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clean up texts\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download() Download nltk data for first time use (download all packages)\n",
    "import nltk.data\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# stemmer = SnowballStemmer('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    # 0. remove mentions(@), Hashtag(#)\n",
    "    sentence = re.sub(r'@[^\\s]+', '', sentence,flags=re.MULTILINE )\n",
    "    sentence = re.sub(r'#[^\\s]+', '', sentence,flags=re.MULTILINE )\n",
    "    sentence = re.sub(r'RT[^\\s]+', '', sentence,flags=re.MULTILINE )\n",
    "    # 1. drop http\n",
    "    p1=re.compile(r'http?:\\/\\/\\S+', flags=re.DOTALL)\n",
    "    sentence = re.sub(p1, '', sentence)\n",
    "    # 2. drop https\n",
    "    p1=re.compile(r'https?:\\/\\/\\S+', flags=re.DOTALL)\n",
    "    sentence = re.sub(p1, '', sentence)\n",
    "    # 3. Remove non-letters\n",
    "    sentence = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    # 4. Remove all numbers\n",
    "    sentence = re.sub(r'[0-9]+', '', sentence)\n",
    "    # 5. Convert words to lower case and split them\n",
    "    sentence = sentence.lower().split()\n",
    "    # Remove Stop Words\n",
    "    sentence = [word for word in sentence if not word in stop_words]\n",
    "    # 5. Stemming\n",
    "    # sentence = [stemmer.stem(w) for w in sentence] \n",
    "    # 6. Lemmatizing\n",
    "    sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "\n",
    "    #check if returned sentence is blank\n",
    "    if len(sentence)==0:\n",
    "      return np.nan\n",
    "    else:\n",
    "      # 7. Return a sentence of words\n",
    "      sentence_r = ''\n",
    "      for word in sentence:\n",
    "        sentence_r = sentence_r + ' ' + word\n",
    "      return(sentence_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The numbers are in, and the world still loves Adele—even at her worst https://t.co/RIl3yyFA4V https://t.co/mbI3YbCZrI\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     number world still love adeleeven worst\n",
       "Name: content2, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content2']= df.content.apply(lambda x: sentence_to_wordlist(x))\n",
    "df['content2'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['content2','btc-usddirection'] #,'btc-usddirection','tsladirection','gspcdirection','ixicdirection']\n",
    "df_1 = df[col]\n",
    "df_1 = df_1.dropna()\n",
    "X = df_1['content2']\n",
    "y = df_1[col[1]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10086, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22868, 20887)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22868, 20887)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4475928112291749"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4297520661157025"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=0.001,random_state=42)),\n",
    "                        ])\n",
    "\n",
    "_ = text_clf_svm.fit(X_train, y_train)\n",
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5fff4187167d39f490158a3105a8bcfc6b45dc563f2a068c566e07d07c54b822"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
