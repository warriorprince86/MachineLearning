{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='data/df_raw.csv'\n",
    "df = pd.read_csv('data/df_raw.csv')\n",
    "col=[i.replace(' ','') for i in df.columns]\n",
    "col=[i.replace('=','') for i in col]\n",
    "col=[str.lower(i) for i in col]\n",
    "df.columns=col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clean up texts\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download() Download nltk data for first time use (download all packages)\n",
    "import nltk.data\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# stemmer = SnowballStemmer('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    # 0. remove mentions(@), Hashtag(#)\n",
    "    sentence = re.sub(r'@[^\\s]+', '', sentence,flags=re.MULTILINE )\n",
    "    sentence = re.sub(r'#[^\\s]+', '', sentence,flags=re.MULTILINE )\n",
    "    sentence = re.sub(r'RT[^\\s]+', '', sentence,flags=re.MULTILINE )\n",
    "    # 1. drop http\n",
    "    p1=re.compile(r'http?:\\/\\/\\S+', flags=re.DOTALL)\n",
    "    sentence = re.sub(p1, '', sentence)\n",
    "    # 2. drop https\n",
    "    p1=re.compile(r'https?:\\/\\/\\S+', flags=re.DOTALL)\n",
    "    sentence = re.sub(p1, '', sentence)\n",
    "    # 3. Remove non-letters\n",
    "    sentence = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    # 4. Remove all numbers\n",
    "    sentence = re.sub(r'[0-9]+', '', sentence)\n",
    "    # 5. Convert words to lower case and split them\n",
    "    sentence = sentence.lower().split()\n",
    "    # Remove Stop Words\n",
    "    sentence = [word for word in sentence if not word in stop_words]\n",
    "    # 5. Stemming\n",
    "    # sentence = [stemmer.stem(w) for w in sentence] \n",
    "    # 6. Lemmatizing\n",
    "    sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "\n",
    "    #check if returned sentence is blank\n",
    "    if len(sentence)==0:\n",
    "      return np.nan\n",
    "    else:\n",
    "      # 7. Return a sentence of words\n",
    "      sentence_r = ''\n",
    "      for word in sentence:\n",
    "        sentence_r = sentence_r + ' ' + word\n",
    "      return(sentence_r)\n",
    "  \n",
    "df['content2']= df.content.apply(lambda x: sentence_to_wordlist(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test=df.iloc[:50,:].content2.dropna().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['content2','gspcdirection'] #,'btc-usddirection','tsladirection','gspcdirection','ixicdirection']\n",
    "df_1 = df[col]\n",
    "df_1 = df_1.dropna()\n",
    "X = df_1['content2']\n",
    "y = df_1[col[1]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10086, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2):\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[:train_end]\n",
    "    validate = df.iloc[train_end:validate_end]\n",
    "    test = df.iloc[validate_end:]\n",
    "    return train, validate, test\n",
    "\n",
    "train, validate, test = train_validate_test_split(df_1,train_percent=0.7, validate_percent=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 21:31:08.642301: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "# load the model from TF HUB\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "\n",
    "# Let's see the returned values using the example above\n",
    "bert_results = bert_model(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)\n",
    "\n",
    "# Let's check that the model runs with the output of the preprocessing model\n",
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "\n",
    "\n",
    "# Loss function: \n",
    "# Let's use losses.BinaryCrossentropy because we are doing a binary classification \n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " preprocessing (KerasLayer)     {'input_word_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128)}                                                          \n",
      "                                                                                                  \n",
      " BERT_encoder (KerasLayer)      {'default': (None,   28763649    ['preprocessing[0][0]',          \n",
      "                                512),                             'preprocessing[0][1]',          \n",
      "                                 'encoder_outputs':               'preprocessing[0][2]']          \n",
      "                                 [(None, 128, 512),                                               \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512)],                                               \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 512),                                                       \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 512)}                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['BERT_encoder[0][5]']           \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 1)            513         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,764,162\n",
      "Trainable params: 28,764,161\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "epochs = 5\n",
    "steps_per_epoch = 100\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the BERT model and training\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Epoch 1/5\n",
      "43/43 [==============================] - 2376s 55s/step - loss: -0.0873 - binary_accuracy: 7.9842e-04 - val_loss: 1.9145 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "43/43 [==============================] - 2564s 60s/step - loss: -0.0898 - binary_accuracy: 7.9842e-04 - val_loss: 1.9145 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      " 5/43 [==>...........................] - ETA: 50:26 - loss: -1.9041 - binary_accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=X_train,\n",
    "                                y = y_train,\n",
    "                                batch_size=500,\n",
    "                               validation_split=0.10,\n",
    "                               epochs=epochs,\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss over time\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5fff4187167d39f490158a3105a8bcfc6b45dc563f2a068c566e07d07c54b822"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
